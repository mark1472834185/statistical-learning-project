{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE 7300 - Assignment 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE 7300: Statistical learning in engineering\n",
      "\n",
      "\n",
      "Homework - 10\n",
      "Student name :shucheng zhang\n",
      "Student Email :zhang.shuche@northeastern.edu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Percentage of Effort Contributed by Student : 100%\n",
      "Submission Date: 2023-12-04\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "studentName=\"shucheng zhang\"\n",
    "studentEmail=\"zhang.shuche@northeastern.edu\"\n",
    "homework=10\n",
    "contributedPercentage=100\n",
    "print(\"IE 7300: Statistical learning in engineering\")\n",
    "print(\"\\n\")\n",
    "print(f'Homework - {homework}')\n",
    "print(f'Student name :{studentName}')\n",
    "print(f'Student Email :{studentEmail}')\n",
    "print(\"\\n\"*15)\n",
    "print(f'Percentage of Effort Contributed by Student : {contributedPercentage}%')    \n",
    "print(f'Submission Date: {date.today()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "#Preprocessing methods\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from sklearn.base import clone\n",
    "import copy\n",
    "from scipy import stats\n",
    "from abc import ABC,abstractmethod\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter</th>\n",
       "      <th>x-box</th>\n",
       "      <th>y-box</th>\n",
       "      <th>width</th>\n",
       "      <th>high</th>\n",
       "      <th>onpix</th>\n",
       "      <th>x-bar</th>\n",
       "      <th>y-bar</th>\n",
       "      <th>x2bar</th>\n",
       "      <th>y2bar</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybr</th>\n",
       "      <th>xy2br</th>\n",
       "      <th>x-ege</th>\n",
       "      <th>xegvy</th>\n",
       "      <th>y-ege</th>\n",
       "      <th>yegvx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>J</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter  x-box  y-box  width  high  onpix  x-bar  y-bar  x2bar  y2bar  xybar  \\\n",
       "0      T      2      8      3     5      1      8     13      0      6      6   \n",
       "1      I      5     12      3     7      2     10      5      5      4     13   \n",
       "2      D      4     11      6     8      6     10      6      2      6     10   \n",
       "3      N      7     11      6     6      3      5      9      4      6      4   \n",
       "4      G      2      1      3     1      1      8      6      6      6      6   \n",
       "5      S      4     11      5     8      3      8      8      6      9      5   \n",
       "6      B      4      2      5     4      4      8      7      6      6      7   \n",
       "7      A      1      1      3     2      1      8      2      2      2      8   \n",
       "8      J      2      2      4     4      2     10      6      2      6     12   \n",
       "9      M     11     15     13     9      7     13      2      6      2     12   \n",
       "\n",
       "   x2ybr  xy2br  x-ege  xegvy  y-ege  yegvx  \n",
       "0     10      8      0      8      0      8  \n",
       "1      3      9      2      8      4     10  \n",
       "2      3      7      3      7      3      9  \n",
       "3      4     10      6     10      2      8  \n",
       "4      5      9      1      7      5     10  \n",
       "5      6      6      0      8      9      7  \n",
       "6      6      6      2      8      7     10  \n",
       "7      2      8      1      6      2      7  \n",
       "8      4      8      1      6      1      7  \n",
       "9      1      9      8      1      1      8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the letter recognition csv file\n",
    "column_names = [\"letter\", \"x-box\", \"y-box\", \"width\", \"high\", \n",
    "                \"onpix\", \"x-bar\", \"y-bar\", \"x2bar\", \"y2bar\", \n",
    "                \"xybar\", \"x2ybr\", \"xy2br\", \"x-ege\", \"xegvy\",\n",
    "                \"y-ege\", \"yegvx\"]\n",
    "letter = pd.read_csv(\"letter-recognition.csv\", names = column_names)\n",
    "letter.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "letter    0\n",
       "x-box     0\n",
       "y-box     0\n",
       "width     0\n",
       "high      0\n",
       "onpix     0\n",
       "x-bar     0\n",
       "y-bar     0\n",
       "x2bar     0\n",
       "y2bar     0\n",
       "xybar     0\n",
       "x2ybr     0\n",
       "xy2br     0\n",
       "x-ege     0\n",
       "xegvy     0\n",
       "y-ege     0\n",
       "yegvx     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking null values.\n",
    "letter.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values, qualified dataset no extra action needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Nerual net class\n",
    "class NeuralNet():\n",
    "    '''\n",
    "    A multiclass neural network with custom SGD and metrics\n",
    "    '''\n",
    "    # Initialize default parameters\n",
    "    def __init__(self, layers=[13, 8, 3], learning_rate=0.001, iterations=100, activation='relu'):\n",
    "        self.params = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.loss = []\n",
    "        self.layers = layers\n",
    "        self.activation_func = activation\n",
    "    \n",
    "    # Initialize weights\n",
    "    def init_weights(self):\n",
    "        np.random.seed(1)\n",
    "        self.params[\"W1\"] = np.random.randn(self.layers[0], self.layers[1]) \n",
    "        self.params['b1'] = np.random.randn(self.layers[1],)\n",
    "        self.params['W2'] = np.random.randn(self.layers[1], self.layers[2]) \n",
    "        self.params['b2'] = np.random.randn(self.layers[2],)\n",
    "    \n",
    "    # Define relu activation function\n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    # relu derivatives\n",
    "    def dRelu(self, Z):\n",
    "        Z[Z <= 0] = 0\n",
    "        Z[Z > 0] = 1\n",
    "        return Z\n",
    "\n",
    "    # Define softmax activation function in the out layer\n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z))\n",
    "        return expZ / expZ.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Define tanh activation function\n",
    "    def tanh(self, Z):\n",
    "        return np.tanh(Z)\n",
    "\n",
    "    # tanh derivatives\n",
    "    def dTanh(self, Z):\n",
    "        return 1 - np.tanh(Z) ** 2\n",
    "    \n",
    "    # Define sigmoid activation function\n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # sigmoid derivatives\n",
    "    def dSigmoid(self, Z):\n",
    "        sig = self.sigmoid(Z)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    # Initialize forward propagation\n",
    "    def forward_propagation(self):\n",
    "        Z1 = self.X.dot(self.params['W1']) + self.params['b1']\n",
    "\n",
    "        if self.activation_func == 'relu':\n",
    "            A1 = self.relu(Z1)\n",
    "        elif self.activation_func == 'tanh':\n",
    "            A1 = self.tanh(Z1)\n",
    "        elif self.activation_func == 'sigmoid':\n",
    "            A1 = self.sigmoid(Z1)\n",
    "        elif self.activation_func == 'softmax': \n",
    "            A1 = self.softmax(Z1)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        yhat = self.softmax(Z2)\n",
    "        loss = self.entropy_loss(self.y, yhat)\n",
    "        self.params['Z1'], self.params['A1'], self.params['Z2'] = Z1, A1, Z2\n",
    "        return yhat, loss\n",
    "\n",
    "    # Entropy loss function calculation\n",
    "    def entropy_loss(self, y, yhat):\n",
    "        nsample = len(y)\n",
    "        yhat_clipped = np.clip(yhat, 1e-8, 1 - 1e-8)\n",
    "        loss = -1 / nsample * (y * np.log(yhat_clipped) + (1 - y) * np.log(1 - yhat_clipped)).sum()\n",
    "        return loss\n",
    "\n",
    "    # Initialize backward propagation function\n",
    "    def back_propagation(self, yhat):\n",
    "        # Compute the derivative of the loss with respect to the output of the network\n",
    "        dl_wrt_z2 = yhat - self.y\n",
    "\n",
    "        # Compute gradients for the second layer\n",
    "        dl_wrt_A1 = dl_wrt_z2.dot(self.params['W2'].T)\n",
    "        dl_wrt_w2 = self.params['A1'].T.dot(dl_wrt_z2)\n",
    "        dl_wrt_b2 = np.sum(dl_wrt_z2, axis=0)\n",
    "\n",
    "        # Compute gradients for the first layer\n",
    "        if self.activation_func == 'relu':\n",
    "            dl_wrt_z1 = dl_wrt_A1 * self.dRelu(self.params['Z1'])\n",
    "        elif self.activation_func == 'tanh':\n",
    "            dl_wrt_z1 = dl_wrt_A1 * self.dTanh(self.params['Z1'])\n",
    "        elif self.activation_func == 'sigmoid':\n",
    "            dl_wrt_z1 = dl_wrt_A1 * self.dSigmoid(self.params['Z1'])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "        dl_wrt_w1 = self.X.T.dot(dl_wrt_z1)\n",
    "        dl_wrt_b1 = np.sum(dl_wrt_z1, axis=0)\n",
    "\n",
    "        # Update the weights and biases\n",
    "        self.params['W1'] -= self.learning_rate * dl_wrt_w1\n",
    "        self.params['W2'] -= self.learning_rate * dl_wrt_w2\n",
    "        self.params['b1'] -= self.learning_rate * dl_wrt_b1\n",
    "        self.params['b2'] -= self.learning_rate * dl_wrt_b2\n",
    "\n",
    "    # Initialize the fit function\n",
    "    def fit(self, X, y, optimizer='sgd'):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.init_weights()\n",
    "\n",
    "        if optimizer == 'sgd':\n",
    "            for i in range(self.iterations):\n",
    "                for j in range(X.shape[0]):\n",
    "                    self.X = X[j:j+1]\n",
    "                    self.y = y[j:j+1]\n",
    "                    yhat, loss = self.forward_propagation()\n",
    "                    self.back_propagation(yhat)\n",
    "                    self.loss.append(loss)\n",
    "        elif optimizer == 'gd':\n",
    "            for i in range(self.iterations):\n",
    "                yhat, loss = self.forward_propagation()\n",
    "                self.back_propagation(yhat)\n",
    "                self.loss.append(loss)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    # Initialize the predict function\n",
    "    def predict(self, X):\n",
    "        Z1 = X.dot(self.params['W1']) + self.params['b1']\n",
    "        A1 = self.relu(Z1) if self.activation_func == 'relu' else self.tanh(Z1)\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        pred = self.softmax(Z2)\n",
    "        return np.round(pred)\n",
    "\n",
    "    # Accuracy calculation function\n",
    "    def accuracy(self, y, yhat):\n",
    "        acc = np.sum(y == yhat) / len(y)\n",
    "        return acc\n",
    "\n",
    "    # Confusion matrix generation function\n",
    "    def compute_confusion_matrix(self, true, predictions):\n",
    "        K = len(np.unique(true))  # Number of classes \n",
    "        result = np.zeros((K, K))\n",
    "        for i in range(len(true)):\n",
    "            result[true[i]][predictions[i]] += 1\n",
    "        return result\n",
    "\n",
    "    # Classification report generation function\n",
    "    def compute_classification_report(self, true, predictions):\n",
    "        matrix = self.compute_confusion_matrix(true, predictions)\n",
    "        num_classes = matrix.shape[0]\n",
    "        precision = np.zeros(num_classes)\n",
    "        recall = np.zeros(num_classes)\n",
    "        f1_score = np.zeros(num_classes)\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            if sum(matrix[:, i]) == 0:\n",
    "                precision[i] = np.nan  # Avoid division by zero\n",
    "            else:\n",
    "                precision[i] = matrix[i, i] / sum(matrix[:, i])\n",
    "\n",
    "            if sum(matrix[i, :]) == 0:\n",
    "                recall[i] = np.nan  # Avoid division by zero\n",
    "            else:\n",
    "                recall[i] = matrix[i, i] / sum(matrix[i, :])\n",
    "\n",
    "            if (precision[i] + recall[i]) == 0:\n",
    "                f1_score[i] = np.nan  # Avoid division by zero\n",
    "            else:\n",
    "                f1_score[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "\n",
    "        # Calculate average values, ignoring NaNs\n",
    "        avg_precision = np.nanmean(precision)\n",
    "        avg_recall = np.nanmean(recall)\n",
    "        avg_f1_score = np.nanmean(f1_score)\n",
    "\n",
    "        report = {\"average_precision\": avg_precision, \"average_recall\": avg_recall, \"average_f1-score\": avg_f1_score}\n",
    "        return report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r:\\Anaconda\\envs\\myenv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Features and Lables\n",
    "X = letter.drop(columns=[\"letter\"]) \n",
    "y = letter[\"letter\"]\n",
    "\n",
    "\n",
    "# Scaling dataset\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the dataset again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert Pandas Series to NumPy array and then reshape\n",
    "y_train_array = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test_array = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train_array)\n",
    "y_test_encoded = encoder.transform(y_test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with [16, 5, 26] layers, relu activation, sgd optimizer\n",
      "Accuracy: 0.52075\n",
      "Confusion Matrix:\n",
      " [[148.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 69.  80.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 37.   0.  79.   0.   2.   2.   0.   0.   0.   0.  11.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   2.   2.   1.   1.   0.   0.   0.]\n",
      " [ 50.   1.   0.  98.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.\n",
      "    0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 59.   0.   0.   0.  79.   0.   1.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 53.   1.   0.   5.   0.  69.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "    0.   5.   0.   1.   1.   4.   0.   0.   0.   0.   0.   0.]\n",
      " [116.   0.   4.   0.   5.   0.  32.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.]\n",
      " [108.   0.   0.   5.   0.   0.   0.  20.   0.   0.   2.   0.   1.   0.\n",
      "    1.   0.   0.   3.   0.   0.   1.   0.   0.   1.   2.   0.]\n",
      " [ 43.   4.   0.   0.   0.   0.   0.   0.  93.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   3.   2.   0.]\n",
      " [ 44.   0.   0.   0.   0.   0.   0.   0.   0. 101.   0.   3.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 66.   0.   0.   0.   2.   0.   0.   0.   0.   0.  59.   0.   0.   0.\n",
      "    0.   0.   0.   2.   0.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [ 26.   0.   0.   0.   2.   0.   1.   0.   0.   0.   3. 119.   0.   1.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   0.   2.   0.   0.]\n",
      " [ 22.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 137.   6.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   3.   0.   0.   0.]\n",
      " [ 49.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   6.  76.\n",
      "    0.   0.   0.   0.   0.   0.  14.   0.   1.   0.   0.   0.]\n",
      " [116.   0.   0.   5.   0.   1.   0.   1.   0.   0.   1.   0.   0.   0.\n",
      "   17.   1.   0.   0.   0.   0.   2.   0.   1.   0.   0.   0.]\n",
      " [ 23.   0.   0.   0.   0.  10.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 136.   0.   0.   0.   0.   0.   0.   0.   0.   4.   0.]\n",
      " [131.   0.   0.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.  25.   0.   0.   0.   0.   4.   0.   0.   3.   0.]\n",
      " [115.   6.   0.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.  35.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [123.   3.   0.   0.   2.   0.   0.   0.   2.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   2.   7.   0.   0.   0.   0.   0.   0.  31.]\n",
      " [ 39.   0.   0.   0.   0.   7.   0.   0.   1.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   2.   3. 103.   0.   0.   0.   0.   5.   1.]\n",
      " [ 47.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.   2.  13.\n",
      "    0.   0.   0.   0.   0.   0. 117.   0.   2.   0.   0.   0.]\n",
      " [ 22.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0. 128.   3.   0.   4.   0.]\n",
      " [ 23.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   4. 116.   0.   0.   0.]\n",
      " [118.   0.   0.   0.   1.   0.   0.   0.   0.   0.   4.   4.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0.  26.   0.   0.]\n",
      " [ 47.   0.   0.   0.   0.   3.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   3.   0.   0.   0.  12.   0.  11.   0.   0.  92.   0.]\n",
      " [ 38.   0.   0.   0.   3.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  91.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.8197381396530343, 'average_recall': 0.5212658859035278, 'average_f1-score': 0.5759825286428366}\n",
      "Model with [16, 10, 26] layers, relu activation, sgd optimizer\n",
      "Accuracy: 0.69825\n",
      "Confusion Matrix:\n",
      " [[142.   0.   0.   0.   0.   0.   0.   0.   0.   1.   1.   3.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   0.]\n",
      " [ 54.  96.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 30.   0.  91.   0.   6.   0.   0.   0.   0.   0.   4.   0.   0.   0.\n",
      "    0.   0.   4.   0.   0.   0.   2.   0.   0.   0.   0.   0.]\n",
      " [ 36.   5.   0. 107.   0.   0.   0.   1.   0.   0.   0.   0.   1.   2.\n",
      "    3.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 34.   0.   1.   0. 104.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   1.   0.   0.]\n",
      " [ 39.   0.   0.   0.   0.  86.   0.   0.   6.   0.   0.   0.   0.   0.\n",
      "    0.   3.   0.   2.   2.   1.   0.   0.   1.   0.   0.   0.]\n",
      " [ 56.   1.   5.   0.   0.   0.  96.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 73.   3.   0.   5.   0.   0.   0.  54.   0.   0.   2.   0.   0.   2.\n",
      "    0.   0.   0.   2.   0.   1.   0.   0.   0.   2.   0.   0.]\n",
      " [ 23.   1.   0.   0.   0.   0.   1.   0. 113.   0.   0.   2.   0.   0.\n",
      "    0.   1.   1.   0.   1.   0.   0.   0.   0.   1.   0.   2.]\n",
      " [ 24.   1.   0.   0.   0.   2.   0.   0.   2. 113.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   2.]\n",
      " [ 38.   0.   0.   1.   1.   0.   0.   3.   0.   0.  78.   0.   0.   2.\n",
      "    0.   0.   0.   2.   0.   0.   0.   1.   0.   4.   0.   0.]\n",
      " [ 28.   1.   0.   0.   1.   0.   2.   0.   0.   0.   0. 118.   0.   0.\n",
      "    1.   0.   3.   0.   0.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [  7.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   2. 155.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   3.   0.   0.   0.]\n",
      " [ 17.   0.   0.   7.   0.   0.   0.   0.   0.   0.   3.   0.   1. 121.\n",
      "    0.   0.   0.   0.   0.   0.   1.   1.   0.   0.   0.   0.]\n",
      " [ 43.   0.   0.   4.   0.   0.   0.   1.   0.   1.   0.   0.   0.   0.\n",
      "   88.   0.   3.   0.   0.   0.   0.   0.   5.   0.   0.   0.]\n",
      " [ 17.   4.   0.   0.   0.  10.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    2. 139.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 53.   0.   0.   1.   1.   0.   8.   0.   0.   1.   0.   1.   0.   0.\n",
      "   11.   0.  84.   0.   1.   1.   0.   3.   0.   0.   0.   1.]\n",
      " [ 50.   5.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.   0.   0.\n",
      "    1.   0.   0. 102.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 73.   3.   0.   0.   8.   0.   1.   0.   1.   0.   0.   1.   0.   0.\n",
      "    0.   0.   3.   1.  60.   0.   0.   0.   0.   1.   3.  16.]\n",
      " [ 24.   0.   0.   0.   0.   0.   1.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   2. 128.   0.   0.   0.   0.   4.   3.]\n",
      " [ 27.   0.   0.   1.   0.   0.   0.   4.   0.   0.   0.   0.   3.   4.\n",
      "    0.   0.   0.   0.   0.   0. 142.   0.   2.   0.   0.   0.]\n",
      " [ 16.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   1.   0.   0.   0.   0.   0. 133.   3.   0.   4.   0.]\n",
      " [  5.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   4.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   2. 135.   0.   0.   0.]\n",
      " [ 47.   0.   0.   2.   2.   0.   0.   0.   3.   0.   1.   1.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.  96.   0.   1.]\n",
      " [ 31.   0.   0.   0.   0.   3.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   8.   0.   8.   0.   2. 114.   2.]\n",
      " [ 22.   0.   0.   0.   3.   0.   0.   0.   0.   3.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   5.   0.   0.   0.   0.   1.   0.  98.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.8524466091392355, 'average_recall': 0.6976683556659304, 'average_f1-score': 0.7458904617458011}\n",
      "Model with [16, 15, 26] layers, relu activation, sgd optimizer\n",
      "Accuracy: 0.77775\n",
      "Confusion Matrix:\n",
      " [[142.   0.   0.   1.   0.   0.   0.   0.   1.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   2.   1.   0.   0.   0.   0.   0.   1.   0.]\n",
      " [ 33. 107.   0.   8.   1.   0.   0.   2.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 16.   0.  93.   0.   7.   0.   7.   0.   0.   0.   5.   0.   0.   0.\n",
      "    5.   0.   1.   0.   0.   2.   1.   0.   0.   0.   0.   0.]\n",
      " [ 17.   4.   0. 132.   0.   0.   0.   0.   0.   1.   0.   0.   0.   1.\n",
      "    0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 21.   0.   1.   0. 111.   0.   1.   0.   0.   0.   2.   3.   0.   0.\n",
      "    0.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 26.   1.   0.   1.   3. 103.   1.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   1.   1.   0.   0.   1.   0.   0.   0.]\n",
      " [ 28.   2.   4.   1.   1.   0. 111.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   4.   2.   2.   0.   0.   4.   0.   0.   0.   0.]\n",
      " [ 39.   0.   0.  10.   0.   0.   0.  76.   0.   0.   3.   0.   0.   3.\n",
      "    2.   0.   0.  10.   0.   0.   1.   0.   0.   0.   0.   0.]\n",
      " [ 17.   0.   0.   2.   0.   1.   1.   0. 117.   3.   0.   1.   0.   0.\n",
      "    0.   1.   1.   0.   0.   0.   0.   0.   0.   1.   1.   0.]\n",
      " [ 21.   0.   0.   1.   0.   1.   0.   0.   2. 122.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.]\n",
      " [ 25.   0.   0.   1.   0.   0.   0.   0.   0.   0.  88.   0.   0.   0.\n",
      "    0.   0.   0.  12.   0.   0.   0.   0.   0.   4.   0.   0.]\n",
      " [ 17.   0.   1.   0.   4.   0.   3.   0.   0.   0.   1. 125.   0.   0.\n",
      "    0.   0.   1.   2.   0.   1.   0.   0.   0.   0.   0.   0.]\n",
      " [ 11.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 152.   4.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.]\n",
      " [ 19.   0.   0.   3.   0.   1.   0.   0.   0.   0.   2.   0.   5. 118.\n",
      "    0.   0.   0.   1.   0.   1.   0.   0.   1.   0.   0.   0.]\n",
      " [ 22.   0.   0.   7.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "  109.   1.   0.   0.   0.   0.   0.   0.   4.   0.   0.   0.]\n",
      " [  9.   2.   0.   0.   0.  13.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1. 142.   0.   0.   0.   1.   0.   0.   0.   0.   3.   0.]\n",
      " [ 35.   1.   0.   1.   1.   0.   1.   0.   0.   0.   0.   2.   0.   0.\n",
      "    5.   0. 114.   2.   1.   0.   0.   0.   0.   0.   2.   1.]\n",
      " [ 31.   2.   0.   4.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0. 121.   0.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 43.   5.   0.   0.   5.   1.   4.   0.   1.   1.   0.   1.   0.   0.\n",
      "    0.   0.   1.   0.  97.   1.   0.   0.   0.   0.   3.   8.]\n",
      " [ 13.   0.   0.   1.   0.   2.   0.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   0.   2. 138.   0.   0.   0.   1.   0.   4.]\n",
      " [ 19.   0.   0.   0.   0.   0.   0.   0.   0.   2.   1.   0.   0.   1.\n",
      "    5.   0.   0.   0.   0.   0. 153.   0.   2.   0.   0.   0.]\n",
      " [ 13.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   1.   0. 135.   6.   0.   2.   0.]\n",
      " [  6.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   1.\n",
      "    1.   0.   0.   0.   0.   0.   1.   3. 135.   0.   0.   0.]\n",
      " [ 24.   0.   0.   1.   1.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "    0.   0.   0.   1.   0.   1.   0.   0.   0. 124.   0.   1.]\n",
      " [ 15.   0.   0.   1.   0.   3.   0.   0.   1.   2.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   2.   6.   0.   1.   0.   4. 133.   0.]\n",
      " [  8.   0.   0.   0.   3.   0.   0.   0.   0.   3.   0.   0.   0.   0.\n",
      "    0.   0.   0.   1.   4.   0.   0.   0.   0.   0.   0. 113.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.86720880263427, 'average_recall': 0.7767912216498529, 'average_f1-score': 0.8048488033011642}\n",
      "Model with [16, 25, 26] layers, relu activation, sgd optimizer\n",
      "Accuracy: 0.83825\n",
      "Confusion Matrix:\n",
      " [[142.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    2.   0.   0.   0.   1.   0.   0.   0.   0.   0.   1.   1.]\n",
      " [ 34. 112.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   2.   3.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  8.   0. 119.   0.   0.   0.   2.   1.   0.   0.   1.   0.   0.   0.\n",
      "    3.   0.   0.   0.   0.   0.   3.   0.   0.   0.   0.   0.]\n",
      " [ 15.   1.   0. 130.   0.   0.   0.   3.   0.   0.   0.   0.   0.   2.\n",
      "    0.   1.   0.   0.   0.   3.   0.   0.   0.   0.   0.   1.]\n",
      " [ 21.   0.   2.   0. 111.   0.   3.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.   1.]\n",
      " [ 18.   0.   0.   1.   1. 113.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   1.   1.   2.   0.   0.   0.   0.   1.   0.]\n",
      " [ 19.   0.   5.   2.   1.   0. 129.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 33.   0.   0.   5.   0.   0.   0.  89.   0.   0.   2.   0.   0.   1.\n",
      "    2.   0.   1.   8.   0.   0.   0.   0.   0.   2.   1.   0.]\n",
      " [  6.   0.   0.   2.   0.   0.   0.   0. 126.   2.   0.   2.   0.   0.\n",
      "    0.   1.   0.   0.   3.   0.   0.   0.   0.   3.   0.   1.]\n",
      " [ 10.   0.   0.   0.   0.   1.   0.   0.   1. 131.   0.   0.   0.   0.\n",
      "    1.   0.   1.   0.   1.   0.   0.   0.   0.   2.   0.   1.]\n",
      " [ 16.   0.   0.   3.   1.   0.   0.   0.   0.   0. 101.   0.   1.   0.\n",
      "    0.   0.   0.   5.   0.   0.   0.   0.   0.   3.   0.   0.]\n",
      " [ 12.   0.   0.   0.   1.   0.   2.   0.   0.   0.   0. 138.   0.   0.\n",
      "    0.   0.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  8.   2.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0. 157.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  9.   0.   1.   3.   0.   0.   0.   1.   0.   0.   0.   0.   0. 133.\n",
      "    1.   0.   0.   3.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 19.   0.   0.   2.   0.   0.   2.   2.   0.   0.   0.   0.   0.   0.\n",
      "  112.   0.   4.   0.   0.   0.   1.   0.   3.   0.   0.   0.]\n",
      " [  8.   1.   0.   0.   0.  12.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 148.   0.   0.   0.   0.   0.   1.   0.   0.   1.   0.]\n",
      " [ 16.   1.   0.   1.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0. 140.   0.   0.   0.   0.   0.   1.   0.   2.   2.]\n",
      " [ 25.   2.   0.   3.   0.   0.   1.   0.   0.   0.   1.   0.   0.   0.\n",
      "    1.   0.   0. 126.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 25.   1.   0.   1.   1.   0.   0.   0.   0.   1.   0.   2.   0.   0.\n",
      "    0.   0.   2.   0. 125.   0.   0.   0.   0.   0.   5.   8.]\n",
      " [ 10.   0.   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   2.   2. 142.   1.   0.   0.   1.   1.   2.]\n",
      " [ 13.   0.   1.   0.   0.   0.   0.   0.   0.   0.   1.   0.   2.   0.\n",
      "    4.   0.   0.   0.   0.   0. 162.   0.   0.   0.   0.   0.]\n",
      " [ 12.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   1.   0. 138.   3.   0.   3.   0.]\n",
      " [  2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.   2.\n",
      "    0.   0.   1.   0.   0.   0.   2.   1. 138.   0.   0.   0.]\n",
      " [  7.   0.   0.   3.   0.   0.   1.   0.   0.   0.   3.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0. 139.   1.   0.]\n",
      " [  7.   0.   0.   1.   0.   5.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   3.   0.   1.   2.   0.   4.   0.   1. 142.   1.]\n",
      " [ 11.   0.   0.   0.   2.   0.   0.   0.   0.   3.   0.   1.   0.   0.\n",
      "    0.   0.   1.   1.   3.   0.   0.   0.   0.   0.   0. 110.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.8942247698266557, 'average_recall': 0.837063500131108, 'average_f1-score': 0.8553149477391369}\n",
      "Model with [16, 5, 26] layers, tanh activation, sgd optimizer\n",
      "Accuracy: 0.389\n",
      "Confusion Matrix:\n",
      " [[126.   0.   0.   4.   0.   0.   0.   0.   1.  13.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   4.   0.   0.   0.   0.   0.]\n",
      " [128.  15.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   8.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 58.   0.  67.   0.   2.   0.   0.   0.   0.   0.   4.   0.   0.   0.\n",
      "    0.   0.   3.   0.   0.   0.   3.   0.   0.   0.   0.   0.]\n",
      " [ 68.   0.   0.  85.   0.   0.   0.   0.   0.   0.   0.   0.   3.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [133.   0.   0.   0.   6.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 58.   0.   0.   0.   0.  64.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.  14.   0.   0.   0.   2.   0.   0.   0.   0.   1.   0.]\n",
      " [145.   0.  12.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [133.   0.   0.   5.   0.   0.   0.   0.   0.   1.   1.   0.   1.   0.\n",
      "    0.   1.   0.   1.   0.   0.   1.   0.   0.   0.   0.   0.]\n",
      " [ 55.   0.   1.   1.   0.   1.   0.   0.  84.   2.   0.   2.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 96.   0.   0.   0.   0.   1.   0.   0.   6.  45.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 88.   0.   0.   2.   0.   0.   0.   0.   0.   0.  23.   0.   0.   0.\n",
      "    0.   0.   0.  10.   0.   0.   4.   0.   0.   3.   0.   0.]\n",
      " [ 49.   0.   5.   0.   1.   0.   0.   0.   0.   0.   1.  99.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 30.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 134.   3.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.]\n",
      " [ 59.   0.   0.   3.   0.   0.   0.   0.   0.   0.   8.   0.  19.  54.\n",
      "    0.   0.   0.   0.   0.   0.   4.   0.   4.   0.   0.   0.]\n",
      " [137.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    5.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.]\n",
      " [ 25.   0.   0.   0.   0.  24.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 124.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [134.   0.   0.   1.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "    0.   0.  22.   0.   0.   0.   0.   4.   0.   0.   4.   0.]\n",
      " [126.   0.   0.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.  30.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [126.   2.   0.   0.   0.   0.   0.   0.   0.   2.   0.   0.   0.   0.\n",
      "    0.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.  39.]\n",
      " [ 83.   0.   0.   0.   0.   3.   0.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.  56.   0.   8.   0.   0.  10.   1.]\n",
      " [ 64.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.   0.\n",
      "    0.   0.   0.   1.   0.   0. 113.   1.   0.   0.   0.   0.]\n",
      " [ 33.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0. 105.  10.   0.   9.   0.]\n",
      " [ 22.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   7. 117.   0.   0.   0.]\n",
      " [116.   0.   0.   0.   2.   0.   0.   0.   0.   0.   2.   2.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   0.  29.   0.   2.]\n",
      " [ 45.   0.   0.   0.   0.   2.   0.   0.   0.   5.   0.   0.   0.   0.\n",
      "    0.   9.   0.   0.   0.  20.   1.   5.   0.   0.  81.   0.]\n",
      " [ 57.   0.   0.   0.   0.   0.   0.   0.   0.   3.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  72.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.7431974488848667, 'average_recall': 0.3857248623811986, 'average_f1-score': 0.4873699197451579}\n",
      "Model with [16, 10, 26] layers, tanh activation, sgd optimizer\n",
      "Accuracy: 0.6765\n",
      "Confusion Matrix:\n",
      " [[142.   0.   0.   0.   0.   0.   0.   0.   0.   4.   0.   1.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   1.   0.]\n",
      " [ 45.  99.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   4.   5.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 26.   0.  95.   0.   1.   0.   5.   0.   0.   0.   7.   0.   1.   0.\n",
      "    1.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.]\n",
      " [ 27.   8.   0. 114.   0.   0.   0.   0.   0.   2.   0.   2.   1.   0.\n",
      "    1.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.]\n",
      " [ 46.   0.   1.   0.  86.   0.   3.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.   2.]\n",
      " [ 34.   3.   0.   1.   0.  97.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   3.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 71.   1.  19.   0.   0.   0.  64.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   1.   1.   0.   0.   0.   2.   0.   0.   0.   0.]\n",
      " [ 77.   0.   0.   5.   0.   0.   0.  36.   0.   0.   4.   0.   0.   3.\n",
      "    4.   3.   1.  10.   0.   0.   1.   0.   0.   0.   0.   0.]\n",
      " [ 26.   1.   1.   1.   0.   0.   0.   0. 110.   1.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   1.   0.   0.   0.   0.   2.   2.   0.]\n",
      " [ 22.   0.   0.   0.   0.   3.   0.   0.   4. 117.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   2.]\n",
      " [ 37.   0.   0.   4.   0.   0.   1.   0.   0.   0.  76.   0.   0.   0.\n",
      "    0.   0.   0.  11.   0.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 20.   0.   0.   1.   4.   0.   3.   0.   0.   0.   0. 122.   0.   0.\n",
      "    0.   0.   1.   0.   4.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 16.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 145.   3.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   3.   0.   0.   0.]\n",
      " [ 28.   0.   0.   1.   0.   0.   0.   2.   0.   0.   6.   0.   2. 102.\n",
      "    3.   0.   0.   0.   0.   3.   0.   2.   2.   0.   0.   0.]\n",
      " [ 55.   0.   0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   1.   0.\n",
      "   79.   1.   4.   0.   0.   0.   0.   0.   1.   1.   0.   0.]\n",
      " [ 17.   1.   0.   0.   0.   9.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1. 141.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.]\n",
      " [ 67.   1.   0.   1.   0.   0.   3.   0.   0.   1.   0.   1.   0.   0.\n",
      "    0.   0.  84.   0.   0.   0.   0.   4.   1.   0.   3.   0.]\n",
      " [ 56.   2.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 101.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 79.   4.   0.   0.   0.   1.   0.   0.   1.   0.   0.   8.   0.   0.\n",
      "    0.   2.   0.   3.  62.   1.   0.   0.   0.   0.   1.   9.]\n",
      " [ 24.   0.   0.   0.   1.   4.   0.   0.   3.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1. 121.   0.   0.   0.   1.   6.   1.]\n",
      " [ 34.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   4.   0.\n",
      "    0.   0.   0.   0.   0.   0. 144.   0.   0.   0.   0.   0.]\n",
      " [ 21.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   3.   0. 125.   2.   0.   6.   0.]\n",
      " [ 17.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.   0.\n",
      "    0.   0.   0.   0.   0.   0.   2.   1. 125.   0.   0.   0.]\n",
      " [ 32.   1.   0.   3.   2.   0.   0.   0.   2.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0. 112.   0.   0.]\n",
      " [ 34.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   9.   0.   6.   0.   2. 115.   0.]\n",
      " [ 23.   0.   0.   0.   3.   0.   0.   0.   0.   4.   0.   0.   0.   0.\n",
      "    0.   0.   0.   1.   9.   0.   0.   0.   0.   0.   0.  92.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.8447812580020865, 'average_recall': 0.675363422911368, 'average_f1-score': 0.7257476186431973}\n",
      "Model with [16, 15, 26] layers, tanh activation, sgd optimizer\n",
      "Accuracy: 0.743\n",
      "Confusion Matrix:\n",
      " [[141.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.   1.   1.   0.\n",
      "    0.   0.   0.   2.   0.   0.   1.   0.   0.   0.   1.   0.]\n",
      " [ 36. 113.   0.   1.   1.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 30.   0.  90.   0.   2.   0.   5.   0.   0.   0.   2.   0.   0.   0.\n",
      "    5.   0.   0.   0.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
      " [ 31.   4.   0. 117.   0.   0.   0.   0.   0.   0.   0.   0.   3.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 29.   0.   0.   0. 105.   0.   2.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.   1.]\n",
      " [ 34.   0.   0.   0.   3.  99.   1.   2.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 53.   0.   6.   0.   0.   0.  90.   0.   0.   0.   0.   3.   0.   0.\n",
      "    0.   0.   6.   2.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 47.   1.   0.   5.   0.   1.   0.  75.   0.   0.   2.   0.   2.   1.\n",
      "    1.   1.   0.   7.   0.   0.   1.   0.   0.   0.   0.   0.]\n",
      " [ 25.   2.   0.   0.   0.   1.   0.   0. 110.   0.   0.   1.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   1.   3.   2.]\n",
      " [ 25.   0.   0.   0.   0.   1.   0.   0.   1. 119.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.]\n",
      " [ 26.   0.   1.   2.   0.   0.   0.   1.   0.   0.  85.   0.   0.   1.\n",
      "    0.   0.   0.  12.   0.   0.   0.   0.   0.   2.   0.   0.]\n",
      " [ 29.   1.   0.   0.   2.   0.   3.   0.   0.   0.   0. 118.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 13.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 145.   4.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   5.   0.   0.   0.]\n",
      " [ 25.   0.   0.   1.   0.   0.   0.   1.   0.   0.   2.   0.   1. 115.\n",
      "    2.   0.   0.   1.   0.   0.   0.   0.   3.   0.   0.   0.]\n",
      " [ 37.   0.   0.   4.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "   94.   0.   6.   0.   0.   0.   0.   0.   2.   0.   0.   0.]\n",
      " [ 20.   1.   0.   0.   0.   9.   2.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0. 136.   0.   0.   0.   0.   0.   0.   0.   0.   4.   0.]\n",
      " [ 33.   2.   0.   1.   0.   0.   3.   0.   0.   0.   0.   0.   0.   0.\n",
      "    6.   0. 119.   1.   0.   0.   0.   0.   0.   0.   0.   1.]\n",
      " [ 44.   4.   0.   3.   0.   0.   1.   2.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0. 105.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 56.   7.   0.   0.   2.   3.   0.   0.   0.   1.   0.   6.   0.   0.\n",
      "    0.   0.   0.   2.  85.   1.   0.   0.   0.   0.   1.   7.]\n",
      " [ 27.   0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1. 132.   0.   0.   0.   0.   0.   1.]\n",
      " [ 12.   0.   1.   0.   0.   0.   0.   0.   0.   0.   1.   0.   4.   3.\n",
      "   10.   0.   0.   0.   0.   1. 149.   0.   2.   0.   0.   0.]\n",
      " [ 14.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0. 140.   1.   0.   1.   0.]\n",
      " [  9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   1. 137.   0.   0.   0.]\n",
      " [ 17.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.   0. 134.   0.   0.]\n",
      " [ 32.   0.   0.   0.   0.   3.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   5.   0.   6.   0.   1. 120.   1.]\n",
      " [ 22.   0.   1.   0.   6.   0.   0.   0.   0.   2.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.  99.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.8739784146793931, 'average_recall': 0.7420635766237544, 'average_f1-score': 0.7854776248242119}\n",
      "Model with [16, 25, 26] layers, tanh activation, sgd optimizer\n",
      "Accuracy: 0.79975\n",
      "Confusion Matrix:\n",
      " [[144.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   0.\n",
      "    0.   0.   1.   0.   1.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 36. 109.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   4.   2.   0.   0.   0.   0.   0.   1.   0.]\n",
      " [ 18.   0. 103.   0.   3.   1.   3.   1.   0.   0.   1.   1.   0.   0.\n",
      "    3.   0.   1.   0.   0.   0.   2.   0.   0.   0.   0.   0.]\n",
      " [ 19.   1.   0. 129.   0.   2.   0.   1.   0.   0.   0.   0.   1.   0.\n",
      "    1.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 16.   0.   3.   0. 104.   1.   6.   0.   0.   0.   5.   3.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   0.   1.   0.   1.]\n",
      " [ 17.   0.   3.   0.   0. 111.   1.   0.   2.   0.   0.   0.   0.   0.\n",
      "    0.   2.   1.   0.   0.   3.   0.   0.   0.   0.   0.   0.]\n",
      " [ 27.   0.   2.   1.   0.   0. 122.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   1.   3.   2.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 32.   2.   1.   4.   0.   0.   0.  82.   0.   1.   2.   0.   0.   1.\n",
      "    5.   1.   0.  12.   0.   0.   1.   0.   0.   0.   0.   0.]\n",
      " [  8.   0.   0.   1.   0.   2.   0.   0. 120.   6.   0.   2.   0.   0.\n",
      "    0.   1.   0.   0.   3.   0.   0.   0.   0.   3.   0.   0.]\n",
      " [ 13.   0.   0.   0.   0.   3.   0.   0.   2. 128.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   1.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 11.   1.   0.   1.   1.   0.   0.   3.   0.   0.  98.   0.   0.   0.\n",
      "    0.   0.   0.  12.   0.   0.   0.   0.   0.   3.   0.   0.]\n",
      " [ 20.   0.   1.   0.   2.   0.   1.   1.   0.   0.   0. 126.   0.   1.\n",
      "    0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   2.]\n",
      " [ 21.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 141.   1.\n",
      "    0.   0.   0.   0.   0.   0.   2.   0.   3.   0.   0.   0.]\n",
      " [ 16.   0.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.   2. 123.\n",
      "    2.   0.   0.   2.   0.   0.   1.   0.   3.   0.   0.   0.]\n",
      " [ 24.   0.   0.   2.   0.   0.   0.   1.   0.   1.   0.   0.   1.   0.\n",
      "  112.   0.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 14.   1.   0.   0.   0.  12.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "    0. 143.   0.   0.   0.   1.   0.   0.   0.   0.   1.   0.]\n",
      " [ 26.   3.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    2.   0. 129.   1.   1.   0.   0.   0.   0.   0.   1.   2.]\n",
      " [ 30.   3.   0.   6.   1.   0.   1.   1.   0.   0.   4.   0.   0.   0.\n",
      "    1.   0.   0. 111.   0.   0.   0.   0.   0.   2.   0.   0.]\n",
      " [ 36.   1.   0.   0.   1.   1.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   3. 116.   1.   1.   1.   0.   0.   1.   8.]\n",
      " [  7.   1.   0.   0.   0.   1.   0.   0.   1.   1.   1.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0. 144.   1.   2.   0.   0.   1.   2.]\n",
      " [ 14.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.   1.\n",
      "    1.   0.   0.   0.   0.   0. 163.   0.   1.   0.   0.   0.]\n",
      " [ 16.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0. 135.   2.   0.   2.   0.]\n",
      " [  4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.\n",
      "    0.   0.   0.   0.   0.   0.   1.   1. 140.   0.   0.   0.]\n",
      " [ 19.   0.   0.   0.   1.   2.   0.   0.   2.   0.   5.   1.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0. 122.   0.   1.]\n",
      " [ 14.   0.   0.   1.   0.   4.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   1.   0.   2.   6.   0.   4.   0.   0. 133.   2.]\n",
      " [ 16.   0.   0.   0.   3.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0. 111.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.876911924103728, 'average_recall': 0.7988120178957854, 'average_f1-score': 0.8240411907447436}\n",
      "Model with [16, 5, 26] layers, sigmoid activation, sgd optimizer\n",
      "Accuracy: 0.40225\n",
      "Confusion Matrix:\n",
      " [[ 62.   0.   0.   2.   0.   0.   0.   1.   1.   0.   0.   0.  10.   2.\n",
      "    0.   2.   0.  66.   0.   1.   0.   0.   0.   0.   0.   2.]\n",
      " [  2.  15.   0.   8.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0. 121.   0.   0.   0.   0.   0.   0.   0.   6.]\n",
      " [ 53.   0.   2.   0.  54.   0.   3.   9.   0.   0.   0.   0.   4.   0.\n",
      "    0.   0.   0.   2.   0.   2.   3.   0.   0.   0.   0.   5.]\n",
      " [  3.   1.   0. 128.   0.   0.   0.   2.   0.   0.   0.   0.   3.   1.\n",
      "    0.   3.   0.  15.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 24.  16.   0.   0.  44.   0.   2.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.  22.   0.   1.   0.   0.   0.   0.   0.  31.]\n",
      " [  7.   1.   0.   4.   0.  46.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.  46.   0.  16.   0.  13.   0.   1.   1.   0.   0.   4.]\n",
      " [ 72.   2.   0.   3.  10.   0.  27.   3.   0.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.  35.   0.   0.   0.   3.   0.   0.   0.   3.]\n",
      " [ 15.   0.   0.  28.   0.   1.   0.  68.   0.   0.   0.   0.   3.   1.\n",
      "    0.   5.   0.  21.   0.   0.   1.   1.   0.   0.   0.   0.]\n",
      " [ 11.   1.   0.   2.   2.  16.   0.   0.  61.   0.   0.   1.   0.   0.\n",
      "    0.   3.   0.   6.   0.  10.   0.   0.   0.   0.   0.  33.]\n",
      " [  9.   2.   0.  55.   0.  13.   0.   0.  11.  13.   0.   0.   0.   0.\n",
      "    0.   3.   0.   5.   0.   0.  34.   0.   0.   0.   0.   4.]\n",
      " [ 61.   0.   0.   7.   0.   0.   5.   7.   0.   0.   3.   0.   3.   1.\n",
      "    0.   0.   0.  39.   0.   0.   4.   0.   0.   0.   0.   0.]\n",
      " [ 44.   1.   0.  16.   3.   0.   1.   3.   0.   0.   1.  69.   0.   0.\n",
      "    0.   0.   0.  14.   0.   0.   0.   0.   0.   0.   0.   3.]\n",
      " [  5.   0.   0.   1.   0.   0.   0.   2.   0.   0.   0.   0. 153.   2.\n",
      "    0.   0.   0.   4.   0.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [ 17.   0.   0.  12.   0.   0.   0.  14.   0.   0.   0.   0.  41.  33.\n",
      "    0.   1.   0.   1.   0.   0.   1.   0.  31.   0.   0.   0.]\n",
      " [ 14.   0.   0.  29.   0.   0.   0.  64.   0.   0.   0.   0.   1.   1.\n",
      "    0.   0.   0.  23.   0.   0.  10.   3.   0.   0.   0.   0.]\n",
      " [  1.   0.   0.   1.   0.  16.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 136.   0.  10.   0.   2.   0.   7.   0.   0.   0.   0.]\n",
      " [ 66.   5.   0.   2.   0.   0.   5.  17.   0.   0.   0.   0.   0.   1.\n",
      "    0.   1.   0.  30.   0.   0.   1.  26.   0.   0.   2.  10.]\n",
      " [  3.   0.   0.  19.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 133.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 10.  17.   0.   3.   3.   3.   0.   1.   0.   0.   0.   1.   0.   0.\n",
      "    0.   2.   0.  27.   0.   1.   0.   3.   0.   0.   0. 100.]\n",
      " [  4.   0.   0.   4.   0.   1.   1.   6.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   3.   0. 135.   0.   1.   0.   0.   0.   8.]\n",
      " [ 13.   0.   0.   4.   0.   0.   0.  60.   0.   0.   0.   0.  18.   7.\n",
      "    0.   0.   0.   1.   0.   0.  77.   2.   1.   0.   0.   0.]\n",
      " [  3.   0.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.   1.   0.\n",
      "    0.   1.   0.   7.   0.   5.   0. 135.   4.   0.   0.   0.]\n",
      " [  3.   0.   0.   0.   0.   0.   0.   3.   0.   0.   0.   0.   5.   5.\n",
      "    0.   0.   0.   0.   0.   0.   0.   4. 128.   0.   0.   0.]\n",
      " [ 22.   7.   0.  22.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.  36.   0.   3.  15.   0.   0.   0.   0.  46.]\n",
      " [ 24.   0.   0.   2.   0.  15.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.  65.   2.  28.   0.   0.  27.   3.]\n",
      " [  1.   5.   0.   0.   1.   4.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   7.   0.   0.   0.   0.   0.   0.   0. 114.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.5771285648437278, 'average_recall': 0.39782382238020675, 'average_f1-score': 0.4103380923229214}\n",
      "Model with [16, 15, 26] layers, sigmoid activation, sgd optimizer\n",
      "Accuracy: 0.65725\n",
      "Confusion Matrix:\n",
      " [[136.   0.   0.   0.   0.   0.   1.   0.   0.   0.   2.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   2.   6.   0.   1.]\n",
      " [ 17.  60.   0.  35.   3.   0.   8.   0.   0.   0.   3.   0.   0.   1.\n",
      "    0.   3.   0.   0.   1.   0.   0.   0.  16.   6.   0.   0.]\n",
      " [  6.   0.  68.   0.   1.   0.  35.   0.   0.   0.   5.   0.   0.   0.\n",
      "   10.   0.   0.   0.   0.   0.   5.   0.   7.   0.   0.   0.]\n",
      " [  1.   0.   0. 140.   0.   0.   3.   0.   0.   0.   0.   0.   0.   4.\n",
      "    1.   4.   0.   0.   0.   0.   0.   0.   3.   0.   0.   0.]\n",
      " [  8.   0.   0.   1.  90.   0.  24.   0.   1.   0.   5.   2.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   5.]\n",
      " [  8.   0.   0.   6.   1.  95.   8.   1.   1.   0.   0.   0.   0.   0.\n",
      "    0.   4.   0.   0.   0.   0.   0.   0.   6.   5.   5.   0.]\n",
      " [  1.   0.   0.   4.   0.   0. 139.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.  11.   3.   0.   0.]\n",
      " [ 22.   0.   0.  16.   0.   1.   7.  38.   0.   0.   9.   0.   1.  10.\n",
      "    3.   1.   0.   2.   0.   0.   7.   0.  24.   0.   1.   2.]\n",
      " [  5.   0.   0.   7.   0.   1.   7.   0. 113.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.   0.   0.   0.   0.   0.   0.   5.   3.   3.]\n",
      " [ 34.   0.   0.   2.   0.   5.   0.   0.  11.  90.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0.   0.   0.   4.   0.   2.]\n",
      " [  5.   0.   0.   5.   0.   0.  17.   0.   0.   0.  82.   0.   0.   3.\n",
      "    0.   0.   0.   6.   0.   0.   2.   0.   5.   5.   0.   0.]\n",
      " [ 11.   0.   0.   0.   1.   0.  12.   0.   1.   0.   2. 120.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   4.   4.   0.   0.]\n",
      " [  5.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0. 120.   4.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.  38.   0.   0.   0.]\n",
      " [  6.   0.   0.   2.   0.   0.   1.   0.   0.   0.   1.   0.   0. 107.\n",
      "    2.   0.   0.   0.   0.   0.   0.   0.  31.   1.   0.   0.]\n",
      " [ 16.   0.   0.  10.   0.   0.   3.   4.   0.   0.   0.   0.   0.   2.\n",
      "  100.   0.   0.   0.   0.   0.   1.   0.   9.   0.   0.   0.]\n",
      " [  1.   0.   0.   1.   0.  18.  10.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 132.   0.   0.   0.   1.   0.   0.   7.   1.   2.   0.]\n",
      " [ 11.   0.   0.   4.   2.   0.  73.   0.   0.   0.   0.   0.   0.   0.\n",
      "   12.   0.  55.   0.   0.   0.   0.   0.   2.   0.   3.   4.]\n",
      " [ 29.   0.   0.  17.   0.   0.  17.   0.   0.   0.  21.   1.   2.   3.\n",
      "    0.   1.   0.  55.   0.   0.   0.   0.  12.   2.   0.   0.]\n",
      " [ 43.   2.   0.   9.   1.   2.  13.   3.   6.   1.   0.   2.   0.   0.\n",
      "    2.   4.   0.   0.  16.   0.   1.   0.   0.  21.   5.  40.]\n",
      " [  8.   0.   0.   4.   1.  10.   5.   0.   0.   0.   3.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0. 105.   0.   3.   2.  11.  10.   1.]\n",
      " [  5.   0.   0.   2.   0.   0.   2.   0.   0.   0.   0.   0.   0.   2.\n",
      "   10.   0.   0.   1.   0.   0. 140.   0.  20.   1.   0.   0.]\n",
      " [  3.   0.   0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0. 128.  24.   0.   1.   0.]\n",
      " [  2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0. 145.   0.   0.   0.]\n",
      " [  4.   0.   0.   4.   1.   0.   1.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   2.   0.   0. 140.   0.   0.]\n",
      " [  7.   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   3.   0.  34.   5.   2. 115.   0.]\n",
      " [ 16.   0.   0.   0.   7.   0.   1.   0.   2.   2.   0.   0.   0.   0.\n",
      "    0.   0.   1.   1.   0.   0.   0.   0.   0.   2.   0. 100.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.7721125772014825, 'average_recall': 0.6586772923581425, 'average_f1-score': 0.6557544057714639}\n",
      "Model with [16, 25, 26] layers, sigmoid activation, sgd optimizer\n",
      "Accuracy: 0.67975\n",
      "Confusion Matrix:\n",
      " [[125.   1.   0.   0.   0.   0.   0.   0.   0.   5.   1.   0.   1.   0.\n",
      "    0.   0.   2.   4.   0.   0.   1.   2.   2.   3.   1.   1.]\n",
      " [  7. 125.   0.   0.   2.   0.   0.   0.   0.   0.   1.   0.   0.   1.\n",
      "    0.   2.   0.   9.   0.   0.   0.   2.   4.   0.   0.   0.]\n",
      " [  6.   0.  91.   0.  12.   0.  11.   0.   0.   0.   5.   0.   0.   0.\n",
      "    4.   0.   1.   0.   0.   0.   0.   5.   2.   0.   0.   0.]\n",
      " [  7.  22.   0. 113.   0.   0.   2.   0.   0.   0.   0.   0.   0.   2.\n",
      "    0.   3.   0.   3.   0.   0.   0.   0.   2.   2.   0.   0.]\n",
      " [  6.   4.   3.   0. 119.   0.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.   2.   0.   0.   0.   0.   0.   2.   0.   1.]\n",
      " [  5.   4.   0.   0.   4.  67.   2.   0.   3.   0.   0.   0.   0.   0.\n",
      "    0.  38.   0.   0.   0.   2.   0.   5.   4.   1.   4.   1.]\n",
      " [  7.   2.   2.   1.   2.   0. 120.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   2.   1.   6.   0.   0.   0.   3.  10.   4.   0.   0.]\n",
      " [ 14.   9.   0.   5.   0.   0.   0.  51.   0.   0.   3.   0.   0.  10.\n",
      "    4.   2.   0.  17.   0.   0.   2.   5.  14.   6.   2.   0.]\n",
      " [  3.   4.   0.   2.   1.   0.   0.   0. 120.   2.   0.   0.   0.   0.\n",
      "    0.   4.   0.   0.   0.   0.   0.   0.   0.   7.   0.   3.]\n",
      " [ 12.   0.   0.   0.   2.   0.   0.   0.  15.  99.   0.   0.   0.   1.\n",
      "    1.   7.   1.   5.   0.   0.   0.   0.   0.   5.   0.   1.]\n",
      " [  5.   5.   0.   2.   4.   0.   0.   0.   0.   0.  81.   0.   0.   0.\n",
      "    0.   0.   0.  16.   0.   0.   0.   1.   5.  11.   0.   0.]\n",
      " [  9.   1.   0.   0.  12.   0.   5.   0.   0.   0.   9. 107.   0.   0.\n",
      "    0.   0.   0.   3.   4.   0.   2.   0.   0.   3.   0.   0.]\n",
      " [  8.   2.   0.   0.   1.   0.   0.   0.   0.   0.   3.   0.  98.   8.\n",
      "    0.   0.   0.   0.   0.   0.   1.   1.  46.   0.   0.   0.]\n",
      " [  9.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  76.\n",
      "    0.   0.   0.   4.   0.   0.   0.   1.  59.   0.   0.   0.]\n",
      " [  8.   0.   0.   6.   0.   0.   4.   2.   0.   0.   0.   0.   0.   0.\n",
      "   85.   5.   7.   3.   0.   0.   4.  12.   7.   2.   0.   0.]\n",
      " [  1.   2.   0.   0.   0.   0.   1.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0. 162.   0.   0.   0.   0.   0.   5.   1.   0.   0.   0.]\n",
      " [ 18.   9.   0.   1.   5.   0.  11.   0.   0.   0.   1.   0.   0.   0.\n",
      "    8.   1.  91.   1.   0.   0.   0.  16.   2.   1.   1.   0.]\n",
      " [  5.   7.   0.   2.   3.   0.   1.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0. 129.   0.   0.   0.   2.   9.   1.   0.   0.]\n",
      " [ 28.  36.   0.   0.   7.   0.   3.   0.  15.   0.   0.   0.   0.   0.\n",
      "    0.   4.   1.  10.   4.   1.   0.   2.   0.  15.   1.  44.]\n",
      " [  4.   3.   0.   0.   1.   4.   0.   1.   5.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0. 123.   1.  15.   0.   1.   1.   2.]\n",
      " [  6.   2.   1.   0.   0.   0.   1.   0.   0.   0.   1.   0.   0.   2.\n",
      "    1.   0.   0.   0.   0.   0. 134.  16.  19.   0.   0.   0.]\n",
      " [  0.   2.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0. 139.  15.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   3.   0. 145.   0.   0.   0.]\n",
      " [  5.   0.   0.   1.   3.   0.   0.   0.   4.   0.   4.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   1.   0.   1.   0. 134.   0.   0.]\n",
      " [  8.   0.   0.   0.   0.   1.   0.   0.   2.   0.   0.   0.   0.   1.\n",
      "    0.   8.   0.   0.   0.   5.   0.  65.   3.   0.  75.   0.]\n",
      " [  7.   2.   0.   0.  11.   0.   0.   0.   2.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   2.   1.   0.   0.   0.   0.   0.   0. 106.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.7463314773244046, 'average_recall': 0.6816914611205616, 'average_f1-score': 0.6729926210093276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2000z\\AppData\\Local\\Temp\\ipykernel_5956\\2652997046.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  return expZ / expZ.sum(axis=1, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with [16, 5, 26] layers, relu activation, gd optimizer\n",
      "Accuracy: 0.03725\n",
      "Confusion Matrix:\n",
      " [[149.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [153.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [137.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [156.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [141.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [140.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [160.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [144.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [146.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [149.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [130.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [155.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [168.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [151.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [145.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [173.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [166.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [160.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [171.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [163.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [183.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [158.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [148.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [168.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [132.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.03725, 'average_recall': 0.038461538461538464, 'average_f1-score': 0.07182453603277898}\n",
      "Model with [16, 10, 26] layers, relu activation, gd optimizer\n",
      "Accuracy: 0.03725\n",
      "Confusion Matrix:\n",
      " [[149.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [153.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [137.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [156.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [141.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [140.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [160.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [144.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [146.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [149.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [130.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [155.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [168.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [151.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [145.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [173.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [166.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [160.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [171.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [163.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [183.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [158.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [148.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [168.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [132.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.03725, 'average_recall': 0.038461538461538464, 'average_f1-score': 0.07182453603277898}\n",
      "Model with [16, 15, 26] layers, relu activation, gd optimizer\n",
      "Accuracy: 0.03725\n",
      "Confusion Matrix:\n",
      " [[149.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [153.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [137.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [156.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [141.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [140.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [160.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [144.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [146.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [149.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [130.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [155.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [168.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [151.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [145.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [173.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [166.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [160.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [171.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [163.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [183.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [158.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [148.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [168.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [132.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.03725, 'average_recall': 0.038461538461538464, 'average_f1-score': 0.07182453603277898}\n",
      "Model with [16, 5, 26] layers, tanh activation, gd optimizer\n",
      "Accuracy: 0.2555\n",
      "Confusion Matrix:\n",
      " [[142.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   2.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [146.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   2.   0.   0.   0.   0.   0.   0.   3.   0.   0.   0.]\n",
      " [ 85.   0.   0.   0.  40.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.  10.   2.   0.   0.   0.   0.]\n",
      " [ 70.   0.   0.  77.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   5.   0.   0.   0.   0.   0.   0.   2.   0.   0.   0.]\n",
      " [ 86.   0.   0.   0.  53.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [101.   0.   0.   1.   0.  31.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.   0.   0.   1.   0.   0.   4.   0.   0.   0.]\n",
      " [152.   0.   0.   0.   3.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0.   1.   3.   0.   0.   0.]\n",
      " [115.   0.   0.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.\n",
      "    1.   1.   0.   0.   0.   0.   5.   1.  15.   0.   0.   0.]\n",
      " [ 34.   0.   0.   0.   0.   0.   0.   0. 111.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 58.   0.   0.   0.   0.   7.   0.   0.   7.  74.   0.   0.   0.   0.\n",
      "    0.   3.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 97.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   6.\n",
      "    1.   0.   0.   0.   0.   0.  17.   5.   4.   0.   0.   0.]\n",
      " [ 76.   0.   0.   0.   1.   0.   0.   0.   9.   3.   0.  61.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   2.   3.   0.   0.   0.   0.]\n",
      " [121.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11.\n",
      "    0.   0.   0.   0.   0.   0.   1.   0.  35.   0.   0.   0.]\n",
      " [ 74.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.  23.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.  53.   0.   0.   0.]\n",
      " [126.   0.   0.   3.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    6.   2.   0.   0.   0.   0.   0.   0.   7.   0.   0.   0.]\n",
      " [ 77.   0.   0.   0.   0.  20.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.  74.   0.   0.   0.   0.   0.   0.   2.   0.   0.   0.]\n",
      " [159.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   4.   1.   0.   0.   0.]\n",
      " [153.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    3.   0.   0.   0.   0.   0.   0.   0.   4.   0.   0.   0.]\n",
      " [159.   0.   0.   2.   0.   1.   0.   0.   1.   1.   0.   0.   0.   0.\n",
      "    0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.]\n",
      " [108.   0.   0.   0.   0.   5.   0.   0.  17.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   8.   0.   0.   0.   0.  25.   0.]\n",
      " [ 64.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   5.   0.  11.\n",
      "    0.   0.   0.   0.   0.   0.  86.   9.   6.   0.   1.   0.]\n",
      " [ 32.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.\n",
      "    0.   1.   0.   0.   0.   0.   0.  84.   7.   0.  32.   0.]\n",
      " [ 13.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.\n",
      "    0.   0.   0.   0.   0.   0.   0.   2. 130.   0.   0.   0.]\n",
      " [118.   0.   0.   8.  13.   0.   0.   0.   1.   1.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   3.   2.   3.   1.   0.   4.   0.]\n",
      " [ 78.   0.   0.   0.   0.   4.   0.   0.   3.   0.   0.   0.   0.   1.\n",
      "    0.  11.   0.   0.   0.   1.   0.   8.   0.   0.  62.   0.]\n",
      " [127.   0.   0.   0.   0.   4.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.5797497570795621, 'average_recall': 0.2543819677611105, 'average_f1-score': 0.430735463700944}\n",
      "Model with [16, 10, 26] layers, tanh activation, gd optimizer\n",
      "Accuracy: 0.58225\n",
      "Confusion Matrix:\n",
      " [[142.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   2.   0.   2.   0.]\n",
      " [107.   0.   0.   3.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   7.   0.  14.  19.   0.   0.   0.   0.   1.   1.   0.]\n",
      " [ 44.   0.  72.   0.   3.   0.  11.   0.   0.   0.   3.   0.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   2.   0.   1.   0.   0.   0.]\n",
      " [ 37.   0.   0. 101.   0.   5.   0.   1.   0.   3.   0.   0.   3.   1.\n",
      "    0.   1.   0.   2.   1.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 36.   0.   0.   0.  86.   0.   5.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   1.   0.  10.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 25.   0.   0.   2.   0. 102.   0.   0.   1.   1.   0.   0.   1.   0.\n",
      "    0.   1.   0.   4.   2.   1.   0.   0.   0.   0.   0.   0.]\n",
      " [ 47.   0.   1.   0.   0.   0. 103.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   1.   3.   4.   0.   0.   0.   1.   0.   0.   0.]\n",
      " [ 90.   0.   0.  12.   1.   1.   3.   0.   0.   1.   4.   0.   6.   3.\n",
      "   15.   1.   0.   6.   0.   0.   1.   0.   0.   0.   0.   0.]\n",
      " [ 24.   0.   0.   0.   0.   2.   2.   0. 106.   2.   0.   2.   0.   0.\n",
      "    1.   1.   0.   0.   2.   0.   0.   0.   0.   4.   0.   0.]\n",
      " [ 15.   0.   0.   0.   0.   4.   0.   0.  15. 107.   0.   2.   0.   0.\n",
      "    2.   0.   0.   0.   0.   0.   0.   0.   0.   4.   0.   0.]\n",
      " [ 43.   0.   0.   0.   0.   0.   5.   0.   0.   0.  58.   0.   3.   5.\n",
      "    0.   0.   1.  12.   0.   0.   1.   0.   1.   0.   1.   0.]\n",
      " [ 24.   0.   0.   0.   0.   0.   4.   0.   0.   0.   0. 122.   0.   0.\n",
      "    0.   0.   0.   1.   1.   0.   0.   0.   0.   3.   0.   0.]\n",
      " [  8.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 158.   1.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.]\n",
      " [ 26.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   9. 105.\n",
      "    4.   0.   0.   1.   0.   0.   4.   0.   0.   0.   1.   0.]\n",
      " [ 43.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   92.   4.   2.   0.   0.   0.   0.   0.   3.   0.   0.   0.]\n",
      " [ 11.   0.   0.   0.   0.  21.   1.   0.   1.   0.   0.   0.   0.   0.\n",
      "    2. 132.   0.   1.   1.   0.   0.   0.   0.   0.   3.   0.]\n",
      " [ 51.   0.   0.   1.   2.   0.  12.   0.   0.   1.   0.   1.   2.   0.\n",
      "    4.   0.  85.   3.   1.   0.   0.   0.   1.   0.   2.   0.]\n",
      " [ 48.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.\n",
      "    0.   0.   0. 107.   2.   0.   0.   0.   0.   1.   0.   0.]\n",
      " [ 59.   0.   0.   0.   5.   0.   2.   0.   1.   0.   0.   1.   0.   0.\n",
      "    0.   0.   0.   4.  94.   0.   0.   0.   0.   0.   5.   0.]\n",
      " [ 45.   0.   0.   2.   3.  29.   0.   0.   8.   0.   0.   0.   4.   1.\n",
      "    0.   0.   0.   0.   0.  41.   1.   1.   0.   0.  28.   0.]\n",
      " [ 44.   0.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   9.   0.\n",
      "    1.   0.   0.   0.   0.   1. 120.   1.   0.   0.   3.   0.]\n",
      " [ 24.   0.   0.   0.   0.   2.   0.   0.   0.   0.   1.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.  77.   4.   0.  49.   0.]\n",
      " [ 14.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   7.   0.\n",
      "    1.   0.   0.   0.   0.   0.   0.   0. 126.   0.   0.   0.]\n",
      " [ 54.   0.   0.   2.  16.   0.   2.   0.   2.   0.   3.   0.   0.   0.\n",
      "    0.   0.   0.   1.  10.   0.   4.   0.   0.  60.   0.   0.]\n",
      " [ 25.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   1.   0.   0.   0.   3.   0. 133.   0.]\n",
      " [ 71.   0.   0.   1.  19.   0.   0.   0.   0.  10.   0.   0.   0.   0.\n",
      "    0.   0.   1.   0.  29.   0.   0.   0.   0.   1.   0.   0.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.7383981120031823, 'average_recall': 0.5760280460183138, 'average_f1-score': 0.6697237035937487}\n",
      "Model with [16, 15, 26] layers, tanh activation, gd optimizer\n",
      "Accuracy: 0.653\n",
      "Confusion Matrix:\n",
      " [[141.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   3.   0.   0.   1.   0.   1.   0.   1.   0.]\n",
      " [ 39. 104.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   6.   1.   0.   0.   0.   0.   0.   1.   0.]\n",
      " [ 21.   0.  68.   0.   2.   0.  31.   0.   0.   0.   1.   0.   3.   0.\n",
      "    4.   0.   0.   0.   0.   1.   3.   0.   3.   0.   0.   0.]\n",
      " [ 28.  10.   0. 106.   0.   0.   0.   0.   0.   1.   0.   1.   1.   3.\n",
      "    1.   3.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 28.   0.   1.   0.  85.   0.  10.   0.   0.   0.   4.   2.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   1.   0.   0.   0.   0.   9.]\n",
      " [ 42.   3.   0.   2.   0.  64.   6.   0.   5.   0.   0.   0.   1.   0.\n",
      "    0.  11.   0.   0.   0.   1.   0.   0.   0.   0.   4.   1.]\n",
      " [ 28.   2.   2.   0.   0.   0. 123.   0.   0.   0.   0.   1.   1.   0.\n",
      "    0.   1.   0.   2.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 67.   0.   0.   8.   0.   0.   4.   5.   0.   1.   1.   0.   1.   7.\n",
      "   35.   1.   0.   6.   0.   0.   3.   0.   1.   0.   2.   2.]\n",
      " [ 21.   2.   0.   2.   0.   0.   0.   0. 110.   2.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.   0.   0.   2.   3.   3.]\n",
      " [ 25.   0.   0.   1.   0.   2.   0.   0.   4. 115.   0.   0.   0.   0.\n",
      "    0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   1.]\n",
      " [ 35.   2.   0.   0.   1.   0.   9.   0.   0.   0.  64.   1.   1.   3.\n",
      "    0.   0.   0.  11.   0.   0.   1.   0.   0.   2.   0.   0.]\n",
      " [ 27.   1.   0.   0.   1.   0.   6.   0.   0.   0.   0. 117.   0.   0.\n",
      "    1.   0.   0.   0.   0.   1.   0.   0.   0.   1.   0.   0.]\n",
      " [ 13.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 145.   6.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   4.   0.   0.   0.]\n",
      " [ 13.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1. 123.\n",
      "    7.   0.   0.   0.   0.   0.   1.   0.   6.   0.   0.   0.]\n",
      " [ 25.   0.   0.   1.   0.   0.   3.   0.   0.   0.   0.   0.   1.   0.\n",
      "  109.   1.   0.   2.   0.   0.   2.   0.   1.   0.   0.   0.]\n",
      " [ 10.   0.   0.   0.   0.   6.   8.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1. 147.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.]\n",
      " [ 48.   2.   0.   1.   1.   0.   8.   0.   0.   0.   0.   3.   0.   0.\n",
      "   21.   1.  76.   4.   0.   0.   0.   0.   0.   0.   1.   0.]\n",
      " [ 57.   3.   0.   3.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "    1.   0.   0.  94.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 62.  16.   0.   0.   4.   0.   3.   0.   1.   0.   0.   4.   0.   0.\n",
      "    0.   2.   0.   8.   5.   2.   0.   0.   0.   0.   5.  59.]\n",
      " [ 25.   2.   0.   2.   0.   2.   1.   0.   0.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0. 115.   0.   0.   0.   0.  11.   3.]\n",
      " [ 19.   0.   0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   3.   0.\n",
      "    6.   0.   1.   0.   0.   0. 151.   0.   1.   0.   0.   0.]\n",
      " [ 19.   0.   0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   3.   0.   0.   0.   0.   0.  68.   8.   0.  58.   0.]\n",
      " [  9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.\n",
      "    0.   0.   0.   0.   0.   0.   2.   0. 135.   0.   0.   0.]\n",
      " [ 46.   2.   0.   1.   2.   0.   0.   0.   1.   2.   2.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   2.   0.   0.  90.   2.   3.]\n",
      " [ 15.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.   0.   0.   2.   0.   1.   1.   1. 143.   2.]\n",
      " [ 13.   0.   0.   0.   1.   0.   1.   0.   0.   8.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 109.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.8065500346926331, 'average_recall': 0.6509982517883177, 'average_f1-score': 0.6643542716712864}\n",
      "Model with [16, 5, 26] layers, sigmoid activation, gd optimizer\n",
      "Accuracy: 0.37775\n",
      "Confusion Matrix:\n",
      " [[125.   2.   0.   1.   0.   0.   0.   0.   1.   0.   0.   1.   3.   0.\n",
      "    0.   1.   0.   9.   0.   0.   0.   1.   2.   0.   1.   2.]\n",
      " [  2.  82.   0.   3.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   6.   0.  59.   0.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [ 94.   1.   1.   2.   2.   1.   4.   5.   0.   0.   0.   0.   3.   0.\n",
      "    0.   0.   0.   0.   0.   0.  10.  11.   1.   0.   0.   2.]\n",
      " [ 11.   3.   0. 102.   0.   1.   0.   4.   0.   0.   0.   0.   3.   0.\n",
      "    0.  15.   0.  15.   0.   0.   0.   0.   0.   0.   0.   2.]\n",
      " [ 51.  21.   0.   2.  11.   0.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.  22.   0.   0.   1.  12.   0.   0.   0.  17.]\n",
      " [  6.  13.   0.   0.   0.  58.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.  54.   0.   1.   0.   1.   0.   1.   2.   0.   1.   3.]\n",
      " [ 87.   6.   0.   1.   1.   0.  12.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   2.   0.  40.   0.   0.   0.  10.   0.   0.   1.   0.]\n",
      " [ 21.   9.   0.  13.   0.   0.   0.  59.   0.   0.   0.   0.   3.   2.\n",
      "    0.   7.   0.  25.   0.   0.   1.   4.   0.   0.   0.   0.]\n",
      " [ 37.  11.   0.   3.   0.  11.   0.   0.  26.   5.   0.   0.   0.   0.\n",
      "    0.   4.   0.   0.   0.   0.   9.   0.   0.   0.  28.  12.]\n",
      " [ 11.  12.   0.  73.   0.   3.   0.   0.   0.  24.   0.   4.   0.   0.\n",
      "    0.   9.   0.   0.   0.   0.   6.   0.   0.   0.   0.   7.]\n",
      " [ 74.   2.   1.   3.   0.   0.   1.   3.   0.   0.   0.   1.   4.   0.\n",
      "    0.   0.   0.  33.   0.   0.   2.   2.   1.   0.   3.   0.]\n",
      " [ 37.   6.   0.   0.   0.   0.   0.   0.   0.  21.   0.  64.   0.   0.\n",
      "    0.   1.   0.   5.   0.   0.   3.   7.   1.   0.   1.   9.]\n",
      " [  6.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 155.   1.\n",
      "    0.   0.   0.   4.   0.   0.   0.   1.   0.   0.   0.   0.]\n",
      " [ 29.   0.   0.   8.   0.   0.   0.   6.   0.   0.   0.   0.  68.  21.\n",
      "    0.   3.   0.   1.   0.   0.   3.   2.  10.   0.   0.   0.]\n",
      " [ 21.   0.   0.  45.   0.   0.   0.  47.   0.   0.   0.   0.   1.   0.\n",
      "    0.   5.   0.  13.   0.   0.   7.   5.   0.   0.   0.   1.]\n",
      " [  1.   5.   0.   1.   0.  13.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 145.   0.   2.   0.   0.   0.   6.   0.   0.   0.   0.]\n",
      " [ 23.  21.   0.   1.   0.   0.   1.  14.   0.   0.   0.   0.   0.   0.\n",
      "    0.   6.   0.  27.   0.   0.   0.  68.   1.   0.   1.   3.]\n",
      " [ 13.   1.   0.  11.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 129.   0.   0.   0.   4.   0.   0.   0.   0.]\n",
      " [ 14.  49.   0.   2.   0.   8.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      "    0.   2.   0.  10.   0.   0.   0.   3.   0.   0.   6.  76.]\n",
      " [ 78.   0.   0.   0.   0.  49.   0.   2.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   6.   0.   0.   0.  12.   0.   0.   8.   8.]\n",
      " [ 23.   0.   0.   2.   0.   0.   0.  35.   0.   0.   0.   0.  18.   1.\n",
      "    0.   1.   0.   3.   0.   1.  94.   5.   0.   0.   0.   0.]\n",
      " [  9.   2.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.   1.   0.\n",
      "    0.   2.   0.   3.   0.   0.   0. 136.   3.   0.   0.   0.]\n",
      " [  6.   0.   0.   0.   0.   0.   0.   2.   0.   0.   0.   0.  15.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.  10. 114.   0.   0.   0.]\n",
      " [ 41.  10.   1.   2.  10.   1.   1.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   4.   0.  10.   0.   1.   5.  29.   0.   0.  14.  24.]\n",
      " [ 54.   2.   0.   1.   0.   9.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   5.   0.   1.   0.   0.   0.  46.   0.   0.  46.   4.]\n",
      " [  7.  12.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   4.   0.   0.   0.   0.   0.   0.   0. 107.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.4812579799992577, 'average_recall': 0.3730884388342639, 'average_f1-score': 0.4012648006214107}\n",
      "Model with [16, 10, 26] layers, sigmoid activation, gd optimizer\n",
      "Accuracy: 0.545\n",
      "Confusion Matrix:\n",
      " [[143.   0.   0.   0.   0.   0.   0.   0.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   0.   2.   0.   0.   0.   1.   0.   1.   0.]\n",
      " [ 18.  82.   0.   0.  21.   0.   5.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.  11.   0.   2.  11.   0.   0.   1.   2.   0.   0.   0.]\n",
      " [  3.   0. 113.   0.   5.   0.   1.   2.   0.   0.   6.   0.   0.   0.\n",
      "    0.   2.   1.   0.   1.   0.   0.   1.   1.   0.   0.   1.]\n",
      " [ 24.   0.   2.  64.   3.   0.   2.   0.   0.   7.   0.   2.   3.   3.\n",
      "   14.  21.   0.   0.  10.   1.   0.   0.   0.   0.   0.   0.]\n",
      " [  6.   0.   8.   0. 113.   0.   8.   0.   0.   0.   2.   1.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   1.   0.   0.   1.]\n",
      " [  8.   2.   1.   0.   4.   8.   5.   0.   2.   0.   1.   0.   0.   0.\n",
      "    0. 101.   0.   0.   5.   2.   0.   0.   1.   0.   0.   0.]\n",
      " [  9.   0.  44.   0.   3.   0.  91.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   2.   0.   3.   0.   0.   1.   6.   0.   1.   0.]\n",
      " [ 23.   1.   8.   1.   9.   0.   7.   7.   0.   1.  10.   1.   5.   3.\n",
      "   41.   2.   2.   2.   0.   0.   2.   0.  18.   0.   1.   0.]\n",
      " [ 11.   0.   3.   0.   0.   0.   1.   0.  91.   4.   0.   1.   0.   9.\n",
      "    0.   6.   8.   0.   6.   0.   0.   0.   0.   0.   2.   4.]\n",
      " [ 18.   0.   4.   0.   4.   2.   0.   0.   4.  99.   0.   0.   0.   0.\n",
      "    0.  11.   3.   0.   4.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 26.   0.  11.   0.   8.   0.  15.   1.   0.   0.  53.   0.   1.   2.\n",
      "    0.   0.   0.   6.   0.   0.   0.   1.   6.   0.   0.   0.]\n",
      " [ 16.   1.   1.   3.  14.   0.   7.   0.   0.   0.   1. 110.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.]\n",
      " [ 19.   0.   1.   0.   1.   0.   1.   5.   0.   0.   6.   0. 117.   3.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.  15.   0.   0.   0.]\n",
      " [ 15.   0.   2.   1.   2.   0.   0.   3.   0.   0.   4.   0.   1.  52.\n",
      "    3.   0.   0.   0.   1.   0.   1.   0.  66.   0.   0.   0.]\n",
      " [ 26.   0.  14.   0.   0.   0.   8.   0.   0.   1.   0.   0.   0.   3.\n",
      "   65.  11.   4.   0.   0.   0.   0.   0.  13.   0.   0.   0.]\n",
      " [  2.   0.   1.   0.   2.   1.   6.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 154.   0.   0.   0.   0.   0.   0.   5.   0.   2.   0.]\n",
      " [ 21.   0.   3.   0.  13.   0.  21.   0.   0.   0.   0.   0.   0.   0.\n",
      "    2.   1.  98.   0.   3.   0.   0.   2.   2.   0.   0.   0.]\n",
      " [ 30.   2.   1.   0.   1.   0.  21.   0.   0.   1.   2.   0.   0.   3.\n",
      "    0.  10.   3.  76.   0.   0.   0.   0.  10.   0.   0.   0.]\n",
      " [ 22.   6.   6.   0.  38.   0.   2.   0.   0.   2.   0.   5.   0.   0.\n",
      "    0.   2.   0.   0.  67.   0.   0.   0.   0.   0.   2.  19.]\n",
      " [ 15.   0.   3.   0.   5.  14.   1.   1.   0.   0.   1.   0.   1.   0.\n",
      "    0.   7.   0.   0.   1.  74.   0.  29.   1.   0.   9.   1.]\n",
      " [ 20.   0.  25.   0.   0.   0.   1.   2.   0.   0.   1.   0.   2.  12.\n",
      "    1.   0.   0.   0.   0.   0.  95.   5.  19.   0.   0.   0.]\n",
      " [  4.   0.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   1.   0.   0.   0.   0. 131.  18.   0.   1.   0.]\n",
      " [  3.   0.   0.   0.   0.   0.   0.   3.   0.   0.   0.   0.   1.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0. 141.   0.   0.   0.]\n",
      " [ 42.   0.  22.   4.  25.   1.  14.   0.   2.   0.   2.  20.   0.   0.\n",
      "    0.   0.   3.   0.   5.   0.   0.   0.   0.   1.  11.   2.]\n",
      " [ 12.   0.   0.   0.   2.   5.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.  13.   4.   0.   3.   1.   0.  53.   0.   0.  74.   0.]\n",
      " [  8.   0.   0.   0.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.  38.   0.   0.   0.   0.   0.   0.  61.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.6426772029043625, 'average_recall': 0.5427771725413045, 'average_f1-score': 0.5274718304010031}\n",
      "Model with [16, 15, 26] layers, sigmoid activation, gd optimizer\n",
      "Accuracy: 0.5875\n",
      "Confusion Matrix:\n",
      " [[127.   0.   0.   0.   0.   0.   3.   0.   0.   0.   1.   0.   0.   4.\n",
      "    1.   0.   0.   2.   0.   0.   0.   0.   1.   4.   6.   0.]\n",
      " [ 10.  93.   0.  15.   0.   4.   5.   2.   0.   0.   1.   0.   0.   1.\n",
      "    0.   1.   0.   0.   0.   0.   0.   0.   3.   4.  14.   0.]\n",
      " [  3.   0.  73.   0.   0.   0.  38.   2.   0.   0.   3.   0.   1.   3.\n",
      "    3.   0.   0.   0.   0.   1.   4.   0.   5.   0.   1.   0.]\n",
      " [  1.   1.   0. 122.   0.   3.   3.   0.   1.   0.   0.   0.   0.   2.\n",
      "    5.   0.   0.   0.   0.   0.   3.   0.   0.   8.   7.   0.]\n",
      " [ 17.   1.   0.   0.  44.   0.  48.   6.   0.   0.   2.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.  11.   8.   3.]\n",
      " [  2.   0.   0.   5.   0.  84.   3.   1.   1.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.   0.   0.   0.   0.   3.  39.   0.]\n",
      " [  4.   0.   0.   1.   0.   0. 137.   0.   0.   0.   3.   0.   0.   0.\n",
      "    0.   0.   0.   1.   0.   0.   0.   0.   5.   5.   4.   0.]\n",
      " [ 18.   0.   0.   9.   0.   0.   7.  38.   0.   0.   3.   0.   1.  10.\n",
      "   22.   0.   0.   5.   0.   0.  12.   0.   0.   3.  16.   0.]\n",
      " [  3.   1.   0.   3.   0.   1.   3.   0. 112.   0.   0.   1.   0.   0.\n",
      "    3.   0.   0.   0.   3.   0.   0.   0.   0.   8.   8.   0.]\n",
      " [ 37.   0.   0.   1.   0.   6.   0.   0.  28.  56.   0.   0.   0.   0.\n",
      "    1.   0.   0.   0.   2.   0.   0.   0.   0.  12.   6.   0.]\n",
      " [ 10.   1.   0.   1.   0.   0.  37.   4.   0.   0.  49.   0.   0.   1.\n",
      "    0.   0.   0.   9.   0.   0.   2.   0.   0.  12.   4.   0.]\n",
      " [ 14.   0.   0.   1.   1.   0.  19.   0.   0.   0.   1. 100.   0.   0.\n",
      "    1.   0.   0.   0.   0.   0.   4.   0.   0.   7.   7.   0.]\n",
      " [  6.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0. 130.  12.\n",
      "    0.   0.   0.   0.   0.   0.   7.   0.  12.   0.   0.   0.]\n",
      " [  8.   0.   0.   1.   0.   0.   0.   0.   0.   0.   1.   0.   0. 124.\n",
      "    5.   1.   0.   0.   0.   0.   1.   0.   4.   1.   5.   0.]\n",
      " [ 11.   0.   0.   6.   0.   0.   7.   2.   0.   0.   0.   0.   0.   1.\n",
      "  110.   0.   0.   0.   0.   0.   1.   0.   5.   0.   2.   0.]\n",
      " [  8.   0.   0.   2.   0.  31.   8.   0.   3.   0.   0.   0.   0.   0.\n",
      "    1.  89.   0.   0.   0.   0.   0.   0.   1.   0.  30.   0.]\n",
      " [ 13.   2.   0.   1.   0.   0.  49.   0.   0.   0.   0.   0.   0.   0.\n",
      "   23.   0.  42.   0.   1.   0.   0.   0.   0.   4.  30.   1.]\n",
      " [ 18.   1.   0.  15.   0.   0.  42.   7.   0.   0.   3.   0.   0.   3.\n",
      "    4.   0.   0.  55.   0.   0.   0.   0.   2.   4.   6.   0.]\n",
      " [ 28.   4.   0.   0.   1.   4.   9.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   3.  19.   0.   0.   0.   0.  25.  77.   1.]\n",
      " [  7.   0.   0.   1.   0.  13.   4.   0.   1.   0.   1.   0.   0.   0.\n",
      "    0.   0.   0.   0.   1.  57.   2.   0.   0.   1.  75.   0.]\n",
      " [  1.   0.   0.   0.   0.   0.   2.   0.   0.   0.   1.   0.   3.   1.\n",
      "    9.   0.   0.   0.   0.   0. 153.   0.   7.   0.   6.   0.]\n",
      " [ 10.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   1.   0.   0.   0.   0.   0.  20.   6.   0. 120.   0.]\n",
      " [  2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.\n",
      "    1.   0.   0.   0.   0.   0.   0.   6. 134.   0.   0.   0.]\n",
      " [  3.   1.   0.   0.   0.   0.   1.   1.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0. 144.   4.   0.]\n",
      " [  1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   1.   0.   0.   1.   1. 164.   0.]\n",
      " [ 14.   0.   0.   0.   0.   0.   2.   0.   1.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   2.   9.   0.   0.   0.   0.  17.  13.  74.]]\n",
      "Classification Report:\n",
      " {'average_precision': 0.740455046813544, 'average_recall': 0.5860725618978857, 'average_f1-score': 0.5860756867304244}\n"
     ]
    }
   ],
   "source": [
    "# Define train fit and predict function\n",
    "def train_and_evaluate_model(X_train, y_train_encoded, X_test, y_test_encoded, layers, activation, optimizer):\n",
    "    model = NeuralNet(layers=layers, activation=activation)\n",
    "    model.fit(X_train, y_train_encoded, optimizer=optimizer)  # Use the modified fit method\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = model.accuracy(y_test_encoded.argmax(axis=1), predictions.argmax(axis=1))\n",
    "    confusion_matrix = model.compute_confusion_matrix(y_test_encoded.argmax(axis=1), predictions.argmax(axis=1))\n",
    "    classification_report = model.compute_classification_report(y_test_encoded.argmax(axis=1), predictions.argmax(axis=1))\n",
    "    print(f\"Model with {layers} layers, {activation} activation, {optimizer} optimizer\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
    "    print(\"Classification Report:\\n\", classification_report)\n",
    "\n",
    "# Updated configurations\n",
    "configurations = [\n",
    "    ([16, 5, 26], 'relu', 'sgd'),\n",
    "    ([16, 10, 26], 'relu', 'sgd'),\n",
    "    ([16, 15, 26], 'relu', 'sgd'),\n",
    "    ([16, 25, 26], 'relu', 'sgd'),\n",
    "    ([16, 5, 26], 'tanh', 'sgd'),\n",
    "    ([16, 10, 26], 'tanh', 'sgd'),\n",
    "    ([16, 15, 26], 'tanh', 'sgd'),\n",
    "    ([16, 25, 26], 'tanh', 'sgd'),\n",
    "    ([16, 5, 26], 'sigmoid', 'sgd'),\n",
    "    ([16, 15, 26], 'sigmoid', 'sgd'),\n",
    "    ([16, 25, 26], 'sigmoid', 'sgd'),\n",
    "    ([16, 5, 26], 'relu', 'gd'),\n",
    "    ([16, 10, 26], 'relu', 'gd'),\n",
    "    ([16, 15, 26], 'relu', 'gd'),\n",
    "    ([16, 5, 26], 'tanh', 'gd'),\n",
    "    ([16, 10, 26], 'tanh', 'gd'),\n",
    "    ([16, 15, 26], 'tanh', 'gd'),\n",
    "    ([16, 5, 26], 'sigmoid', 'gd'),\n",
    "    ([16, 10, 26], 'sigmoid', 'gd'),\n",
    "    ([16, 15, 26], 'sigmoid', 'gd')\n",
    "]\n",
    "\n",
    "\n",
    "# Train and evaluate each configuration\n",
    "for layers, activation, optimizer in configurations:\n",
    "    train_and_evaluate_model(X_train, y_train_encoded, X_test, y_test_encoded, layers, activation, optimizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance metrics outcome analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model with [16, 5, 26] layers, relu activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.52075\n",
    "* `Average Precision:` 0.8197\n",
    "* `Average Recall:` 0.5213\n",
    "* `Average F1-score:` 0.5760\n",
    "* `observation:` Moderate accuracy with balanced precision and recall. This configuration has room for improvement.\n",
    "\n",
    "### **Model with [16, 10, 26] layers, relu activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.69825\n",
    "* `Average Precision:` 0.8524\n",
    "* `Average Recall:` 0.6977\n",
    "* `Average F1-score:` 0.7459\n",
    "* `Observation:` A significant improvement in all metrics compared to the first model. Increasing the middle layer's size seems beneficial.\n",
    "\n",
    "### **Model with [16, 15, 26] layers, relu activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.77775\n",
    "* `Average Precision:` 0.8672\n",
    "* `Average Recall:` 0.7768\n",
    "* `Average F1-score:` 0.8048\n",
    "* `Observation:` Further improvement in performance, indicating that a larger middle layer is more effective for this specific task.\n",
    "\n",
    "### **Model with [16, 25, 26] layers, relu activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.83825\n",
    "* `Average Precision:` 0.8942\n",
    "* `Average Recall:` 0.8371\n",
    "* `Average F1-score:` 0.8553\n",
    "* `Observation:` This configuration achieves the best performance among relu activation models. There's a consistent increase in performance metrics with the increase in the size of the middle layer.\n",
    "\n",
    "### **Model with [16, 5, 26] layers, tanh activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.389\n",
    "* `Average Precision:` 0.7432\n",
    "* `Average Recall:` 0.3857\n",
    "* `Average F1-score:` 0.4874\n",
    "* `Observation:` A significant drop in performance compared to relu models, indicating that tanh activation might not be as effective for this letter dataset.\n",
    "\n",
    "### **Model with [16, 10, 26] layers, tanh activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.6765\n",
    "* `Average Precision:` 0.8448\n",
    "* `Average Recall:` 0.6754\n",
    "* `Average F1-score:` 0.7257\n",
    "* `Observation:` Improvement over the previous tanh model, but still not as effective as relu models.\n",
    "\n",
    "### **Model with [16, 15, 26] layers, tanh activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.743\n",
    "* `Average Precision:` 0.8740\n",
    "* `Average Recall:` 0.7421\n",
    "* `Average F1-score:` 0.7855\n",
    "* `Observation:` The best performing tanh model, but still falls short compared to the best relu models.\n",
    "\n",
    "### **Model with [16, 25, 26] layers, tanh activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.79975\n",
    "* `Average Precision:` 0.8769\n",
    "* `Average Recall:` 0.7988\n",
    "* `Average F1-score:` 0.8240\n",
    "* `Observation:` Closest in performance to relu models but relu still outperforms in overall metrics.\n",
    "\n",
    "### **Model with [16, 5, 26] layers, sigmoid activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.40225\n",
    "* `Average Precision:` 0.5771\n",
    "* `Average Recall:` 0.3978\n",
    "* `Average F1-score:` 0.4103\n",
    "* `Observation:` Low performance, indicating that sigmoid activation is not suitable for this letter dataset as well.\n",
    "\n",
    "### **Model with [16, 15, 26] layers, sigmoid activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.65725\n",
    "* `Average Precision:` 0.7721\n",
    "* `Average Recall:` 0.6587\n",
    "* `Average F1-score:` 0.6558\n",
    "* `Observation:` Moderate improvement but still significantly lower than relu and tanh models.\n",
    "\n",
    "### **Model with [16, 25, 26] layers, sigmoid activation, sgd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.67975\n",
    "* `Average Precision:` 0.7721\n",
    "* `Average Recall:` 0.6798\n",
    "* `Average F1-score:` 0.6780\n",
    "* `Observation:` Slight improvement over the 15-neuron sigmoid model but still not competitive with relu or tanh models.\n",
    "\n",
    "</br>\n",
    "\n",
    "### **Model with [16, 5, 26] layers, relu activation, gd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.03725\n",
    "* `Average Precision:` 0.03725\n",
    "* `Average Recall:` 0.03846\n",
    "* `Average F1-score:` 0.07182\n",
    "* `Observation:` Extremely poor performance, with accuracy and other metrics being significantly low. This suggests that the combination of relu activation and GD is ineffective for this letter dataset.\n",
    "\n",
    "### **Model with [16, 10, 26] layers, relu activation, gd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.03725\n",
    "* `Average Precision:` 0.03725\n",
    "* `Average Recall:` 0.03846\n",
    "* `Average F1-score:` 0.07182\n",
    "* `Observation:` Similar to the previous model, this configuration also shows very poor performance.\n",
    "\n",
    "### **Model with [16, 15, 26] layers, relu activation, gd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.03725\n",
    "* `Average Precision:` 0.03725\n",
    "* `Average Recall:` 0.03846\n",
    "* `Average F1-score:` 0.07182\n",
    "* `Observation:` Consistent with the previous relu models, this configuration also fails to produce satisfactory results.\n",
    "\n",
    "### **Model with [16, 5, 26] layers, tanh activation, gd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.2555\n",
    "* `Average Precision:` 0.5797\n",
    "* `Average Recall:` 0.2543\n",
    "* `Average F1-score:` 0.4307\n",
    "* `Observation:` Shows a moderate increase in performance compared to the relu models, indicating that tanh might be a more suitable activation function when used with GD.\n",
    "\n",
    "### **Model with [16, 10, 26] layers, tanh activation, gd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.58225\n",
    "* `Average Precision:` 0.7383\n",
    "* `Average Recall:` 0.5760\n",
    "* `Average F1-score:` 0.6697\n",
    "* `Observation:` There's a significant improvement in performance with this configuration. This suggests that increasing in the middle layer, combined with tanh activation and GD, can yield better results.\n",
    "\n",
    "### **Model with [16, 15, 26] layers, tanh activation, gd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.653\n",
    "* `Average Precision:` 0.8065\n",
    "* `Average Recall:` 0.6509\n",
    "* `Average F1-score:` 0.6643\n",
    "* `Observation:` Demonstrates the best performance among the tanh models, indicating a positive correlation between the number of neurons in the middle layer and the model's effectiveness when using tanh and GD.\n",
    "\n",
    "### **Model with [16, 5, 26] layers, sigmoid activation, gd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.37775\n",
    "* `Average Precision:` 0.4812\n",
    "* `Average Recall:` 0.3730\n",
    "* `Average F1-score:` 0.4012\n",
    "* `Observation:` Shows better performance than relu models but is still considerably underperforming compared to tanh models, indicating that sigmoid is not the most effective choice for this dataset with GD.\n",
    "\n",
    "### **Model with [16, 10, 26] layers, sigmoid activation, gd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.545\n",
    "* `Average Precision:` 0.6426\n",
    "* `Average Recall:` 0.5427\n",
    "* `Average F1-score:` 0.5274\n",
    "* `Observation:` An improvement in performance is observed with an increased number of neurons, but it still dropped behind the tanh models.\n",
    "\n",
    "### **Model with [16, 15, 26] layers, sigmoid activation, gd optimizer**\n",
    "\n",
    "* `Accuracy:` 0.5875\n",
    "* `Average Precision:` 0.7404\n",
    "* `Average Recall:` 0.5860\n",
    "* `Average F1-score:`  0.5860\n",
    "* `Observation:` Shows the best performance among the sigmoid models, yet it still does not match the effectiveness of the tanh models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluation and Comparison of Model Performance\n",
    "\n",
    "`Hidden Nodes:`\n",
    "\n",
    "* Increase in Hidden Nodes: According to my project dataset: Letter recognition, there's a clear trend that increasing the number of hidden nodes generally improves all performance metrics (accuracy, precision, recall, F1-score) across all activation functions and optimization functions. This is likely due to the model's increased capacity to capture complex patterns in the data.\n",
    "\n",
    "`Activation Functions:`\n",
    "\n",
    "* ReLU (Rectified Linear Unit): Based on the outcome, this specific model with ReLU consistently outperform those with tanh and sigmoid activation functions. ReLU is known for its efficiency in training deep neural networks due to its simplicity and ability to mitigate the vanishing gradient problem.\n",
    "* Tanh (Hyperbolic Tangent): According to the model output, the Tanh models perform better than sigmoid but are less effective than ReLU. Tanh is symmetrical around zero and can handle negative values more effectively than sigmoid, but it seems to fall short of the performance achieved by ReLU in this letter dataset.\n",
    "* Sigmoid: Sigmoid models showed the weakest performance. This might be due to the vanishing gradient problem, as sigmoid activation functions saturate at both ends (0 and 1), leading to small gradients and slow learning.\n",
    "\n",
    "`Optimization Functions:`\n",
    "\n",
    "* Different Types of Gradient Descent: The performance of the models also varied with different types of gradient descent. The differences in performance can be attributed to how each method approaches the optimization process.\n",
    "* Gradient Descent: For GD, it updates the model parameters using the entire dataset at once. While it provides stable error gradients and a steady convergence, it can be computationally intensive and slow, especially with large datasets.\n",
    "* Stochastic Gradient Descent (SGD): For SGD, it updates parameters for each training example. It's faster and can escape local minima better than gradient descent, but it can be more efficient in its path towards convergence.\n",
    "\n",
    "### 2. Interpretation and Discussion of Findings\n",
    "\n",
    "`Impact of Model Complexity:`\n",
    "\n",
    "* The increase in model complexity (more hidden nodes) generally leads to better performance. However, it's also important to balance complexity with the risk of overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "\n",
    "`Activation Function Suitability:`\n",
    "\n",
    "* ReLU's superior performance suggests it's more suitable for this specific letter recognition dataset. This might be due to its linear, non-saturating form, which helps in faster convergence during training.\n",
    "Tanh and sigmoid's lower performance indicates potential issues with vanishing gradients, especially for sigmoid.\n",
    "\n",
    "`Optimization Method Efficacy:`\n",
    "\n",
    "* The choice of gradient descent method significantly impacts model performance. Gradient descent might be too slow and computationally intensive for large datasets, while stochastic gradient descent, though faster, might lead to inconsistent convergence. However, in this case, SGD did a decent job in this model.\n",
    "\n",
    "`General Trends:`\n",
    "\n",
    "* Precision, recall, and F1-scores follow similar trends to accuracy. High precision suggests a low rate of false positives, while high recall indicates a low rate of false negatives. The F1-score provides a balance between precision and recall, which is crucial for imbalanced datasets.\n",
    "\n",
    "`Dataset-Specific Considerations:`\n",
    "\n",
    "* The nature of this dataset also played a significant role in how well each model configuration performs. Certain datasets might inherently favor certain activation functions due to its characteristics, complexities, etc.\n",
    "\n",
    "\n",
    "To conclude, to gain a more comprehensive understanding, experimenting with different optimizers, learning rates, and even more diverse architectures would be beneficial. The findings emphasize the importance of model architecture and parameters' choices tailored to the specific outcome of the dataset. ReLU emerges as the most effective activation function in this experiments, with an increase in hidden nodes correlating with improved performance across all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
